use std::{fmt::Display, fs, path::Path};

use openai_dive::v1::resources::{response::{request::ResponseParametersBuilder, response::ResponseReasoning}, shared::ReasoningEffort};
use serde::Deserialize;

#[derive(Debug, Clone, Copy, PartialEq, Eq)]
pub enum ModelProvider {
    OpenAI,
    GeminiAIStudio,
}

impl ModelProvider {
    fn parse(s: &str) -> Option<Self> {
        match s.trim().to_lowercase().as_str() {
            "openai" => Some(Self::OpenAI),
            "gemini" | "aistudio" | "gemini_aistudio" | "gemini-ai-studio" => Some(Self::GeminiAIStudio),
            _ => None,
        }
    }
}

/// è¨­å®š
/// env ã‚‚ã—ãã¯ config.json ã‹ã‚‰ãƒ­ãƒ¼ãƒ‰ã•ã‚Œã‚‹
#[derive(Clone)]
pub struct Config {
    pub discord_token: String,
    /// åˆ©ç”¨ã™ã‚‹ãƒ¢ãƒ‡ãƒ«ãƒ—ãƒ­ãƒã‚¤ãƒ€
    pub model_provider: ModelProvider,
    /// APIã‚­ãƒ¼ï¼ˆOpenAIãªã‚‰ Bearerã€Gemini AI Studioãªã‚‰ query ã® keyï¼‰
    pub main_model_api_key: String,
    /// OpenAIäº’æ›APIã®ãƒ™ãƒ¼ã‚¹URL (ä¾‹: https://api.openai.com/v1)
    pub main_model_endpoint: String,
    /// ãƒ—ãƒ­ãƒã‚¤ãƒ€å›ºæœ‰ã®ãƒ¢ãƒ‡ãƒ«å (Geminiä¾‹: gemini-flash-latest)
    pub main_model_name: String,
    pub system_prompt: String,
    pub rale_limit_window_size: u64,
    pub rate_limit_sec_per_cost: u64,
    pub web_server_host: [u8; 4],
    pub web_server_local_ip: [u8; 4],
    pub web_server_port: u16,
    pub admin_users: Vec<u64>,
    pub timeout_millis: u64,
}

impl Config {
    pub fn new() -> Self {
        dotenv::dotenv().ok();

        let file_cfg = FileConfig::load_from_default_path();

        let web_server_port = std::env::var("WEB_SERVER_PORT")
            .ok()
            .and_then(|s| s.trim().parse::<u16>().ok())
            .or_else(|| file_cfg.as_ref().and_then(|c| c.web_server_port));

        let discord_token = std::env::var("DISCORD_TOKEN")
            .ok()
            .and_then(non_empty_non_placeholder)
            .or_else(|| file_cfg.as_ref().and_then(|c| c.discord_token.clone()).and_then(non_empty_non_placeholder))
            .expect("DISCORD_TOKEN must be set (env DISCORD_TOKEN or config.json discord_token)");

        let main_model_api_key = std::env::var("MAIN_MODEL_API_KEY")
            .ok()
            .and_then(non_empty_non_placeholder)
            // äº’æ›ã®ãŸã‚æ®‹ã™
            .or_else(|| std::env::var("OPENAI_API_KEY").ok().and_then(non_empty_non_placeholder))
            .or_else(|| {
                file_cfg
                    .as_ref()
                    .and_then(|c| c.model.as_ref())
                    .and_then(|m| m.main_model_api_key.clone())
                    .and_then(non_empty_non_placeholder)
            })
            .expect("MAIN_MODEL_API_KEY must be set (env MAIN_MODEL_API_KEY/OPENAI_API_KEY or config.json model.main_model_api_key)");

        let main_model_endpoint = std::env::var("MAIN_MODEL_ENDPOINT")
            .ok()
            .and_then(non_empty_non_placeholder)
            .or_else(|| {
                file_cfg
                    .as_ref()
                    .and_then(|c| c.model.as_ref())
                    .and_then(|m| m.main_model_endpoint.clone())
                    .and_then(non_empty_non_placeholder)
            })
            .unwrap_or_else(|| "https://api.openai.com/v1".to_string());

        let model_provider = std::env::var("MAIN_MODEL_PROVIDER")
            .ok()
            .and_then(non_empty_non_placeholder)
            .as_deref()
            .and_then(ModelProvider::parse)
            .or_else(|| {
                file_cfg
                    .as_ref()
                    .and_then(|c| c.model.as_ref())
                    .and_then(|m| m.provider.as_deref())
                    .and_then(ModelProvider::parse)
            })
            .unwrap_or_else(|| {
                if main_model_endpoint.contains("generativelanguage.googleapis.com") {
                    ModelProvider::GeminiAIStudio
                } else {
                    ModelProvider::OpenAI
                }
            });

        let main_model_name = std::env::var("MAIN_MODEL_NAME")
            .ok()
            .and_then(non_empty_non_placeholder)
            .or_else(|| {
                file_cfg
                    .as_ref()
                    .and_then(|c| c.model.as_ref())
                    .and_then(|m| m.model_name.clone())
                    .and_then(non_empty_non_placeholder)
            })
            .unwrap_or_else(|| match model_provider {
                ModelProvider::GeminiAIStudio => "gemini-flash-latest".to_string(),
                ModelProvider::OpenAI => "gpt-5-nano".to_string(),
            });

        let system_prompt = std::env::var("SYSTEM_PROMPT").ok().and_then(non_empty_non_placeholder).or_else(|| {
            file_cfg
                .as_ref()
                .and_then(|c| c.prompt.as_ref())
                .and_then(|p| p.ask_developer_prompt.clone())
                .and_then(non_empty_non_placeholder)
        }).unwrap_or_else(||
"ä¸Šè¨˜ã®ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã¯Discordå†…ã§ã®ä¼šè©±ã§ã™ã€‚
æ™‚ç³»åˆ—ã®ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚¿ã‚¤ãƒ ãƒ©ã‚¤ãƒ³ã«ãªã£ã¦ã„ã¦ã€ã‚ãªãŸã¯ã“ã®å†…å®¹ã‹ã‚‰è‡ªç„¶ã«å¿œç­”ã—ã¾ã™ã€‚
ã‚ãªãŸã¯ Discord ã® BOTã€ŒObserverã€ã§ä»¥ä¸Šã®ä¼šè©±ã‚’ç¶šã‘ã¦ãã ã•ã„ã€‚
è‡ªç„¶ã«ä¼šè©±ã—ã€çŸ¥è­˜ç³»ã®è©±é¡Œã§ã¯æƒ…å ±æºã®ç¢ºèªã¨æœ€æ–°æ€§ã®ãƒã‚§ãƒƒã‚¯ã‚’å¿…ãšè¡Œã†ã€‚
æ›–æ˜§ãªæƒ…å ±ã¯èª¿ã¹ã€å†…å®¹ã‚’æé€ ã—ãªã„ã€‚å¿…è¦ãªã‚‰è³ªå•ã—ã¦ã‚‚ã‚ˆã„ã€‚
æƒ…å ±ã¯è«–ç†çš„ã«æ•´ç†ã—ã€å¿…è¦ãŒã‚ã‚Œã° tool ã‚’ä½¿ã£ã¦èª¿æŸ»ã™ã‚‹
tool ã®çµæœã¯ç›¸æ‰‹ã«è¦‹ãˆãªã„ãŸã‚ã€å¿…ãšå†…å®¹ã‚’æ›¸ãå‡ºã™
éåº¦ã«è‡ªä¿¡ã‚’æŒãŸãšã€æ…é‡ã«åˆ¤æ–­ã™ã‚‹
â€œ!â€ ã‚„ emoji ã¯æ§ãˆã‚ã«
æ•°å­¦ã®èª¬æ˜ã§ã¯ latex_expr_render ã‚’ä½¿ã†
discord_send_message ã§ãƒ†ã‚­ã‚¹ãƒˆâ†’æ•°å¼ã®é †ãªã©ã€æ®µéšçš„ã«é€ã‚‹ã®ã‚‚å¯
å†—é•·ãªèª¬æ˜ã‚’é¿ã‘ã€ç«¯çš„ã«
ãƒªã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã¯ ğŸ« ğŸ˜±ğŸ‘ğŸ‘ˆğŸ¤” ã‚’ä¸­å¿ƒã«
ä¸€èªã§ååˆ†ãªã‚‰ä¸€èªã§è¿”ã™ï¼ˆä¾‹:ã€Œã†ã‚“ã€ã€Œãã†ã€ã€Œã¯ï¼Ÿã€ã€Œ???ã€ã€Œ?ã€ã€Œ...ã€ï¼‰
ã‚¹ãƒ©ãƒ³ã‚°ã¯é©åº¦ã«ã€‚åŸºæœ¬ã¯é™ã‹ãƒ»è«–ç†çš„ãƒ»è½ã¡ç€ã„ãŸå£èª¿ã€ãƒ•ãƒ¬ãƒ³ãƒ‰ãƒªãƒ¼ã« æ•¬èªã¯å¿…è¦ãªæ™‚ã®ã¿ã§
ä¸€äººç§°ã¯ã€Œç§ã€ã‹ãª ã¾ãè‡ªç”±ã«
é‡è¦: å‘¨å›²ã®å£èª¿ã‚’çœŸä¼¼ã™ã‚‹ã‚ˆã†ã«ã€‚ã“ã‚Œã¯ã¨ã¦ã‚‚ã‚ˆã„çµæœã‚’ç”Ÿã¿ã¾ã™ã€‚ ãƒ¦ãƒ¼ãƒ¢ã‚¢ã‚’å¤§äº‹ã« èˆˆå‘³æ·±ã„ã‚‚ã®ã«ã¯ãƒªã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã‚’ å¿œç­”ãŒé•·ããªã‚Šã™ããªã„ã‚ˆã†ã«ãƒ†ãƒ³ãƒã‚ˆã
tool_call ã§ãªã„é€šå¸¸ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’é€ã‚‹ã¨æ¨è«–çµ‚äº†ã™ã‚‹ã®ã§æ³¨æ„ã‚’
åŸºæœ¬çš„ã«æœ€å¾Œã®ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã«å¯¾ã—ã¦ç­”ãˆã¦ãã ã•ã„".to_string());
        Config {
            discord_token,
            model_provider,
            main_model_api_key,
            main_model_endpoint,
            main_model_name,
            system_prompt,
            rale_limit_window_size: 16200,
            rate_limit_sec_per_cost: 900,
            web_server_host: [0, 0, 0, 0],
            web_server_local_ip: [192, 168, 0, 26],
            web_server_port: web_server_port.unwrap_or(8096),
            admin_users: vec![855371530270408725],
            timeout_millis: 100_000,
        }
    }
}

fn non_empty_non_placeholder(s: String) -> Option<String> {
    let trimmed = s.trim();
    if trimmed.is_empty() {
        return None;
    }
    if trimmed == "YOUR_API_KEY" {
        return None;
    }
    Some(trimmed.to_string())
}

#[derive(Debug, Clone, Deserialize)]
struct FileConfig {
    #[serde(default)]
    discord_token: Option<String>,
    #[serde(default)]
    web_server_port: Option<u16>,
    #[serde(default)]
    model: Option<FileModelConfig>,
    #[serde(default)]
    prompt: Option<FilePromptConfig>,
}

#[derive(Debug, Clone, Deserialize)]
struct FileModelConfig {
    #[serde(default)]
    main_model_api_key: Option<String>,
    #[serde(default)]
    main_model_endpoint: Option<String>,
    #[serde(default)]
    model_name: Option<String>,
    #[serde(default)]
    provider: Option<String>,
}

#[derive(Debug, Clone, Deserialize)]
struct FilePromptConfig {
    #[serde(default)]
    ask_developer_prompt: Option<String>,
}

impl FileConfig {
    fn load_from_default_path() -> Option<Self> {
        let path = Path::new("config.json");
        let s = fs::read_to_string(path).ok()?;
        serde_json::from_str(&s).ok()
    }
}

impl Default for Config {
    fn default() -> Self {
        Self::new()
    }
}

/// ãƒ¢ãƒ‡ãƒ«ãƒªã‚¹ãƒˆã®å®šç¾©
#[derive(Debug, Clone)]
pub enum Models {
    Gpt5Mini,
    Gpt5Nano,
    Gpt5dot1,
    O4Mini,
    O3,
    Gpt5dot1CodexMini
}

impl From<Models> for String {
    fn from(model: Models) -> Self {
        match model {
            Models::Gpt5Mini => "gpt-5-mini".to_string(),
            Models::Gpt5Nano => "gpt-5-nano".to_string(),
            Models::Gpt5dot1 => "gpt-5.1".to_string(),
            Models::O4Mini => "o4-mini".to_string(),
            Models::O3 => "o3".to_string(),
            Models::Gpt5dot1CodexMini => "gpt-5.1-codex-mini".to_string(),
        }
    }
}

impl Display for Models {
    fn fmt(&self, f: &mut std::fmt::Formatter<'_>) -> std::fmt::Result {
        let model_str: String = self.clone().into();
        write!(f, "{}", model_str)
    }
}

impl From<String> for Models {
    fn from(s: String) -> Models {
        match s.as_str() {
            "gpt-5-mini" => Models::Gpt5Mini,
            "gpt-5-nano" => Models::Gpt5Nano,
            "gpt-5.1" => Models::Gpt5dot1,
            "o4-mini" => Models::O4Mini,
            "o3" => Models::O3,
            "gpt-5.1-codex-mini" => Models::Gpt5dot1CodexMini,
            _ => Models::Gpt5Nano, // default
        }
    }
}

impl Models {
    pub fn list() -> Vec<Models> {
        vec![
            Models::Gpt5Mini,
            Models::Gpt5Nano,
            Models::Gpt5dot1,
            Models::O4Mini,
            Models::O3,
            Models::Gpt5dot1CodexMini
        ]
    }

    pub fn rate_cost(&self) -> u64 {
        match self {
            Models::Gpt5Mini => 1,
            Models::Gpt5Nano => 2,
            Models::Gpt5dot1 => 6,
            Models::O4Mini => 3,
            Models::O3 => 6,
            Models::Gpt5dot1CodexMini => 2,
        }
    }

    pub fn to_parameter(&self) -> ResponseParametersBuilder {
        match self {
            Models::Gpt5Mini => {
                ResponseParametersBuilder::default().model("gpt-5-mini")
                .reasoning(
                    ResponseReasoning {
                        effort: Some(ReasoningEffort::Low),
                    }
                ).clone()
            }
            Models::Gpt5Nano => {
                ResponseParametersBuilder::default().model("gpt-5-nano")
                .reasoning(
                    ResponseReasoning {
                        effort: Some(ReasoningEffort::Low),
                    }
                ).clone()
            }
            Models::Gpt5dot1 => { 
                ResponseParametersBuilder::default().model("gpt-5.1")
                .reasoning(
                    ResponseReasoning {
                        effort: Some(ReasoningEffort::Low),
                    }
                ).clone()
            }
            Models::O4Mini => { 
                ResponseParametersBuilder::default().model("o4-mini")
                .reasoning(
                    ResponseReasoning {
                        effort: Some(ReasoningEffort::Low),
                    }
                ).clone()
            }
            Models::O3 => { 
                ResponseParametersBuilder::default().model("o3")
                .reasoning(
                    ResponseReasoning {
                        effort: Some(ReasoningEffort::Low),
                    }
                ).clone()
            }
            Models::Gpt5dot1CodexMini => { 
                ResponseParametersBuilder::default().model("gpt-5.1-codex-mini")
                .reasoning(
                    ResponseReasoning {
                        effort: Some(ReasoningEffort::Low),
                    }
                ).clone()
            }
        }
    }
}